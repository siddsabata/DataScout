{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from apify_client import ApifyClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load .env variables and initialize apifyclient with API token\n",
    "load_dotenv('.env')\n",
    "APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "# Initalize apify client\n",
    "client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "user_query = input(\"Enter user query: \")\n",
    "\n",
    "# prepare google search actor input\n",
    "run_input = {\n",
    "      \"queries\": user_query,\n",
    "      \"maxPagesPerQuery\": 1,\n",
    "      \"resultsPerPage\": 5,\n",
    "      \"mobileResults\": False,\n",
    "      \"languageCode\": \"\",\n",
    "      \"maxConcurrency\": 10,\n",
    "      \"saveHtml\": False,\n",
    "      \"saveHtmlToKeyValueStore\": False,\n",
    "      \"includeUnfilteredResults\": False,\n",
    "      \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
    "    return {\n",
    "      pageTitle: $('title').text(),\n",
    "    };\n",
    "  };\"\"\",\n",
    "}\n",
    "\n",
    "# Run\n",
    "run = client.actor(\"nFJndFXA5zjCTuudP\").call(run_input=run_input)\n",
    "\n",
    "url_list = []\n",
    "\n",
    "# Creating a url list of websites for content crawling\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).list_items().items[0]['organicResults']:\n",
    "   url_list.append(item['url'])\n",
    "\n",
    "# prepare website content crawler actor input\n",
    "# for now only scraping the first link\n",
    "content_crawler_input = {\n",
    "    \"startUrls\": [{ \"url\": url_list[0]}],\n",
    "    \"useSitemaps\": False,\n",
    "    \"crawlerType\": \"playwright:firefox\",\n",
    "    \"includeUrlGlobs\": [],\n",
    "    \"excludeUrlGlobs\": [],\n",
    "    \"ignoreCanonicalUrl\": False,\n",
    "    \"maxCrawlDepth\": 20,\n",
    "    \"maxCrawlPages\": 10,\n",
    "    \"initialConcurrency\": 0,\n",
    "    \"maxConcurrency\": 200,\n",
    "    \"initialCookies\": [],\n",
    "    \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "    \"maxSessionRotations\": 10,\n",
    "    \"maxRequestRetries\": 3,\n",
    "    \"requestTimeoutSecs\": 60,\n",
    "    \"dynamicContentWaitSecs\": 10,\n",
    "    \"maxScrollHeightPixels\": 5000,\n",
    "    \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "[role=\\\"alert\\\"],\n",
    "[role=\\\"banner\\\"],\n",
    "[role=\\\"dialog\\\"],\n",
    "[role=\\\"alertdialog\\\"],\n",
    "[role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "[aria-modal=\\\"true\\\"]\"\"\",\n",
    "    \"removeCookieWarnings\": True,\n",
    "    \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n",
    "    \"htmlTransformer\": \"readableText\",\n",
    "    \"readableTextCharThreshold\": 100,\n",
    "    \"aggressivePrune\": False,\n",
    "    \"debugMode\": False,\n",
    "    \"debugLog\": False,\n",
    "    \"saveHtml\": False,\n",
    "    \"saveMarkdown\": True,\n",
    "    \"saveFiles\": False,\n",
    "    \"saveScreenshots\": False,\n",
    "    \"maxResults\": 9999999,\n",
    "    \"clientSideMinChangePercentage\": 15,\n",
    "    \"renderingTypeDetectionPercentage\": 10,\n",
    "}\n",
    "\n",
    "# Run the content crawler actor and wait for it to finish\n",
    "website_content_crawler = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input= content_crawler_input)\n",
    "\n",
    "# Fetch and print Actor results from the run's dataset (if there are any)\n",
    "for item in client.dataset(website_content_crawler[\"defaultDatasetId\"]).iterate_items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the dataset into langchain document format\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "from langchain_community.document_loaders.base import Document\n",
    "\n",
    "loader = ApifyDatasetLoader(\n",
    "    dataset_id= website_content_crawler[\"defaultDatasetId\"],\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "\n",
    "# Stores in FAISS vector\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Code for getting summaries from our dataset\n",
    "user_query = input(\"Enter user query: \")\n",
    "result = index.query_with_sources(user_query)\n",
    "\n",
    "print(result[\"answer\"])\n",
    "print(result[\"sources\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascout.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
