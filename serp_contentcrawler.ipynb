{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from apify_client import ApifyClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "from langchain_community.document_loaders.base import Document\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "\n",
    "def google_search_actor(question):\n",
    "    # Returns a list of urls from a question on google search using apify's google search actor\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "    # prepare google search actor input\n",
    "    run_input = {\n",
    "        \"queries\": question,\n",
    "        \"maxPagesPerQuery\": 1,\n",
    "        \"resultsPerPage\": 2,\n",
    "        \"mobileResults\": False,\n",
    "        \"languageCode\": \"\",\n",
    "        \"maxConcurrency\": 10,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveHtmlToKeyValueStore\": False,\n",
    "        \"includeUnfilteredResults\": False,\n",
    "        \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
    "        return {\n",
    "        pageTitle: $('title').text(),\n",
    "        };\n",
    "    };\"\"\",\n",
    "    }\n",
    "\n",
    "    # running the actor\n",
    "    serp = client.actor(\"nFJndFXA5zjCTuudP\").call(run_input=run_input)\n",
    "\n",
    "    url_list = []\n",
    "\n",
    "    # Creating a url list of websites for content crawling\n",
    "    for item in client.dataset(serp[\"defaultDatasetId\"]).list_items().items[0]['organicResults']:\n",
    "        url_list.append(item['url'])\n",
    "        print(\"Adding url to url list:\", item)\n",
    "\n",
    "    print(len(url_list), \"urls\")\n",
    "\n",
    "    return url_list\n",
    "\n",
    "def content_crawler(urls):\n",
    "    # Takes a list of urls and web scrapes using Apify's content crawler\n",
    "    # Load .env variables and initialize apifyclient with API token\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "    \n",
    "    content_crawler_input = {\n",
    "        \"startUrls\": [{\"url\": url} for url in urls],\n",
    "        \"useSitemaps\": False,\n",
    "        \"crawlerType\": \"playwright:firefox\",\n",
    "        \"includeUrlGlobs\": [],\n",
    "        \"excludeUrlGlobs\": [{ \"url\": \"https://www.britannica.com\"}],\n",
    "        \"ignoreCanonicalUrl\": False,\n",
    "        \"maxCrawlDepth\": 5,\n",
    "        \"maxCrawlPages\": 10,\n",
    "        \"initialConcurrency\": 0,\n",
    "        \"maxConcurrency\": 200,\n",
    "        \"initialCookies\": [],\n",
    "        \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "        \"maxSessionRotations\": 10,\n",
    "        \"maxRequestRetries\": 3,\n",
    "        \"requestTimeoutSecs\": 60,\n",
    "        \"dynamicContentWaitSecs\": 10,\n",
    "        \"maxScrollHeightPixels\": 5000,\n",
    "        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "        [role=\\\"alert\\\"],\n",
    "        [role=\\\"banner\\\"],\n",
    "        [role=\\\"dialog\\\"],\n",
    "        [role=\\\"alertdialog\\\"],\n",
    "        [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "        [aria-modal=\\\"true\\\"]\"\"\",\n",
    "        \"removeCookieWarnings\": True,\n",
    "        \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n",
    "        \"htmlTransformer\": \"readableText\",\n",
    "        \"readableTextCharThreshold\": 100,\n",
    "        \"aggressivePrune\": False,\n",
    "        \"debugMode\": False,\n",
    "        \"debugLog\": False,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveMarkdown\": True,\n",
    "        \"saveFiles\": False,\n",
    "        \"saveScreenshots\": False,\n",
    "        \"maxResults\": 9999999,\n",
    "        \"clientSideMinChangePercentage\": 15,\n",
    "        \"renderingTypeDetectionPercentage\": 10,\n",
    "    }\n",
    "\n",
    "    # Run the content crawler actor and wait for it to finish\n",
    "    website_content_crawler = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input= content_crawler_input)\n",
    "\n",
    "    # Loads the dataset into langchain document format\n",
    "    loader = ApifyDatasetLoader(\n",
    "        dataset_id= website_content_crawler[\"defaultDatasetId\"],\n",
    "        dataset_mapping_function=lambda dataset_item: Document(\n",
    "            page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def run_actors(query):\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "    output = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content = f\"\"\"You are an AI language model assistant. Your task is to generate two \n",
    "        different versions of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative Google Search prompts separated by newlines.\n",
    "        Query: {query}\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output_string = output.content\n",
    "    output_list = output_string.split('\\n')\n",
    "\n",
    "    for i in range(len(output_list)):\n",
    "        output_list[i] = output_list[i][3:]\n",
    "\n",
    "    doc_list = []\n",
    "\n",
    "    for question in output_list:\n",
    "        print(\"Querying:\", question)\n",
    "        urls_list = google_search_actor(question)\n",
    "        doc_list.append(content_crawler(urls_list))\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying: \"Explore the impact of Steve Jobs on the technology industry.\"\n",
      "Adding url to url list: {'title': 'How Steve Jobs Changed the World', 'url': 'https://www.investopedia.com/articles/personal-finance/012815/how-steve-jobs-changed-world.asp', 'displayedUrl': 'https://www.investopedia.com › ... › Entrepreneurs', 'description': \"From purchasing Pixar in 1986 to supporting charities and environmental causes, Jobs' achievements and innovations continue to affect industries and lifestyles\\xa0...\", 'emphasizedKeywords': ['affect industries and lifestyles'], 'siteLinks': [], 'productInfo': {}, 'type': 'organic', 'position': 1}\n",
      "Adding url to url list: {'title': \"10 Lessons From the Legacy of Apple's Steve Jobs\", 'url': 'https://spectrum.ieee.org/10-lessons-from-steve-jobs', 'displayedUrl': 'https://spectrum.ieee.org › 10-lessons-from-steve-jobs', 'description': \"Jobs's innovations made a profound impact. He redefined computing, enhancing the user experience, and created products and services loved by\\xa0...\", 'date': '2021-10-27T12:00:00.000Z', 'emphasizedKeywords': ['He redefined computing, enhancing the user experience, and created products and'], 'siteLinks': [], 'productInfo': {}, 'type': 'organic', 'position': 2}\n",
      "2 urls\n",
      "Querying: \"Discover the key innovations and leadership style of Steve Jobs.\"\n",
      "Adding url to url list: {'title': 'The Real Leadership Lessons of Steve Jobs', 'url': 'https://hbr.org/2012/04/the-real-leadership-lessons-of-steve-jobs', 'displayedUrl': 'https://hbr.org › 2012/04 › the-real-leadership-lessons-of...', 'description': \"When Jobs returned, he shifted Apple's focus back to making innovative products: the sprightly iMac, the PowerBook, and then the iPod, the iPhone, and the iPad.\", 'emphasizedKeywords': ['Jobs', 'innovative'], 'siteLinks': [], 'productInfo': {}, 'type': 'organic', 'position': 1}\n",
      "Adding url to url list: {'title': \"Steve Jobs' Leadership Style | MTD ...\", 'url': 'https://www.mtdtraining.com/blog/steve-jobs-leadership-style.htm', 'displayedUrl': 'https://www.mtdtraining.com › blog › steve-jobs-leaders...', 'description': \"Discover Steve Jobs' iconic leadership style. A symphony of passion, vision, and revolutionary drive!\", 'date': '2023-12-18T12:00:00.000Z', 'emphasizedKeywords': ['Discover Steve Jobs', 'leadership style'], 'siteLinks': [], 'productInfo': {}, 'type': 'organic', 'position': 2}\n",
      "2 urls\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Give me a question:\" )\n",
    "\n",
    "x = run_actors(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env variables and initialize apifyclient with API token\n",
    "load_dotenv('.env')\n",
    "APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "# Initalize apify client\n",
    "client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "# prepare google search actor input\n",
    "run_input = {\n",
    "      \"queries\": question,\n",
    "      \"maxPagesPerQuery\": 1,\n",
    "      \"resultsPerPage\": 5,\n",
    "      \"mobileResults\": False,\n",
    "      \"languageCode\": \"\",\n",
    "      \"maxConcurrency\": 10,\n",
    "      \"saveHtml\": False,\n",
    "      \"saveHtmlToKeyValueStore\": False,\n",
    "      \"includeUnfilteredResults\": False,\n",
    "      \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
    "    return {\n",
    "      pageTitle: $('title').text(),\n",
    "    };\n",
    "  };\"\"\",\n",
    "}\n",
    "\n",
    "serp = client.actor(\"nFJndFXA5zjCTuudP\").call(run_input=run_input)\n",
    "\n",
    "url_list = []\n",
    "\n",
    "# Creating a url list of websites for content crawling\n",
    "for item in client.dataset(serp[\"defaultDatasetId\"]).list_items().items[0]['organicResults']:\n",
    "   url_list.append(item['url'])\n",
    "\n",
    "# prepare website content crawler actor input\n",
    "# for now only scraping the first link\n",
    "content_crawler_input = {\n",
    "    \"startUrls\": [{ \"url\": url_list[0]}],\n",
    "    \"useSitemaps\": False,\n",
    "    \"crawlerType\": \"playwright:firefox\",\n",
    "    \"includeUrlGlobs\": [],\n",
    "    \"excludeUrlGlobs\": [],\n",
    "    \"ignoreCanonicalUrl\": False,\n",
    "    \"maxCrawlDepth\": 20,\n",
    "    \"maxCrawlPages\": 10,\n",
    "    \"initialConcurrency\": 0,\n",
    "    \"maxConcurrency\": 200,\n",
    "    \"initialCookies\": [],\n",
    "    \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "    \"maxSessionRotations\": 10,\n",
    "    \"maxRequestRetries\": 3,\n",
    "    \"requestTimeoutSecs\": 60,\n",
    "    \"dynamicContentWaitSecs\": 10,\n",
    "    \"maxScrollHeightPixels\": 5000,\n",
    "    \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "[role=\\\"alert\\\"],\n",
    "[role=\\\"banner\\\"],\n",
    "[role=\\\"dialog\\\"],\n",
    "[role=\\\"alertdialog\\\"],\n",
    "[role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "[aria-modal=\\\"true\\\"]\"\"\",\n",
    "    \"removeCookieWarnings\": True,\n",
    "    \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n",
    "    \"htmlTransformer\": \"readableText\",\n",
    "    \"readableTextCharThreshold\": 100,\n",
    "    \"aggressivePrune\": False,\n",
    "    \"debugMode\": False,\n",
    "    \"debugLog\": False,\n",
    "    \"saveHtml\": False,\n",
    "    \"saveMarkdown\": True,\n",
    "    \"saveFiles\": False,\n",
    "    \"saveScreenshots\": False,\n",
    "    \"maxResults\": 9999999,\n",
    "    \"clientSideMinChangePercentage\": 15,\n",
    "    \"renderingTypeDetectionPercentage\": 10,\n",
    "}\n",
    "\n",
    "# Run the content crawler actor and wait for it to finish\n",
    "website_content_crawler = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input= content_crawler_input)\n",
    "\n",
    "# Loads the dataset into langchain document format\n",
    "loader = ApifyDatasetLoader(\n",
    "    dataset_id= website_content_crawler[\"defaultDatasetId\"],\n",
    "    dataset_mapping_function=lambda dataset_item: Document(\n",
    "        page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Stores in FAISS vector\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vector \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(documents, OpenAIEmbeddings())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Stores in FAISS vector\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Code for getting summaries from our dataset\n",
    "user_query = input(\"Enter user query: \")\n",
    "result = index.query_with_sources(user_query)\n",
    "\n",
    "print(result[\"answer\"])\n",
    "print(result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApifyApiError",
     "evalue": "User was not found or authentication token is not valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApifyApiError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m\n\u001b[1;32m     13\u001b[0m run_input \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mstartUrls\u001b[39m\u001b[39m\"\u001b[39m: [{ \u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mhttps://hbr.org/2012/04/the-real-leadership-lessons-of-steve-jobs\u001b[39m\u001b[39m\"\u001b[39m }],\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39misUrlArticleDefinition\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m}\u001b[39m\u001b[39m\"\"\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     38\u001b[0m \u001b[39m# Run the Actor and wait for it to finish\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m run \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mactor(\u001b[39m\"\u001b[39;49m\u001b[39mlukaskrivka/article-extractor-smart\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcall(run_input\u001b[39m=\u001b[39;49mrun_input)\n\u001b[1;32m     41\u001b[0m \u001b[39m# Fetch and print Actor results from the run's dataset (if there are any)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m client\u001b[39m.\u001b[39mdataset(run[\u001b[39m\"\u001b[39m\u001b[39mdefaultDatasetId\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39miterate_items():\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/_logging.py:82\u001b[0m, in \u001b[0;36m_injects_client_details_to_log_context.<locals>.wrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m log_context\u001b[39m.\u001b[39mclient_method\u001b[39m.\u001b[39mset(fun\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m)\n\u001b[1;32m     80\u001b[0m log_context\u001b[39m.\u001b[39mresource_id\u001b[39m.\u001b[39mset(resource_client\u001b[39m.\u001b[39mresource_id)\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m fun(resource_client, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/clients/resource_clients/actor.py:266\u001b[0m, in \u001b[0;36mActorClient.call\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, webhooks, wait_secs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m: ActorClient,\n\u001b[1;32m    231\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     wait_secs: \u001b[39mint\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Start the actor and wait for it to finish before returning the Run object.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[39m    It waits indefinitely, unless the wait_secs argument is provided.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[39m        dict: The run object\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     started_run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart(\n\u001b[1;32m    267\u001b[0m         run_input\u001b[39m=\u001b[39;49mrun_input,\n\u001b[1;32m    268\u001b[0m         content_type\u001b[39m=\u001b[39;49mcontent_type,\n\u001b[1;32m    269\u001b[0m         build\u001b[39m=\u001b[39;49mbuild,\n\u001b[1;32m    270\u001b[0m         max_items\u001b[39m=\u001b[39;49mmax_items,\n\u001b[1;32m    271\u001b[0m         memory_mbytes\u001b[39m=\u001b[39;49mmemory_mbytes,\n\u001b[1;32m    272\u001b[0m         timeout_secs\u001b[39m=\u001b[39;49mtimeout_secs,\n\u001b[1;32m    273\u001b[0m         webhooks\u001b[39m=\u001b[39;49mwebhooks,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_client\u001b[39m.\u001b[39mrun(started_run[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mwait_for_finish(wait_secs\u001b[39m=\u001b[39mwait_secs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/_logging.py:82\u001b[0m, in \u001b[0;36m_injects_client_details_to_log_context.<locals>.wrapper\u001b[0;34m(resource_client, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m log_context\u001b[39m.\u001b[39mclient_method\u001b[39m.\u001b[39mset(fun\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m)\n\u001b[1;32m     80\u001b[0m log_context\u001b[39m.\u001b[39mresource_id\u001b[39m.\u001b[39mset(resource_client\u001b[39m.\u001b[39mresource_id)\n\u001b[0;32m---> 82\u001b[0m \u001b[39mreturn\u001b[39;00m fun(resource_client, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/clients/resource_clients/actor.py:219\u001b[0m, in \u001b[0;36mActorClient.start\u001b[0;34m(self, run_input, content_type, build, max_items, memory_mbytes, timeout_secs, wait_for_finish, webhooks)\u001b[0m\n\u001b[1;32m    208\u001b[0m run_input, content_type \u001b[39m=\u001b[39m encode_key_value_store_record_value(run_input, content_type)\n\u001b[1;32m    210\u001b[0m request_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params(\n\u001b[1;32m    211\u001b[0m     build\u001b[39m=\u001b[39mbuild,\n\u001b[1;32m    212\u001b[0m     maxItems\u001b[39m=\u001b[39mmax_items,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     webhooks\u001b[39m=\u001b[39mencode_webhook_list_to_base64(webhooks) \u001b[39mif\u001b[39;00m webhooks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    217\u001b[0m )\n\u001b[0;32m--> 219\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhttp_client\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m    220\u001b[0m     url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url(\u001b[39m'\u001b[39;49m\u001b[39mruns\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    221\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    222\u001b[0m     headers\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mcontent-type\u001b[39;49m\u001b[39m'\u001b[39;49m: content_type},\n\u001b[1;32m    223\u001b[0m     data\u001b[39m=\u001b[39;49mrun_input,\n\u001b[1;32m    224\u001b[0m     params\u001b[39m=\u001b[39;49mrequest_params,\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m parse_date_fields(pluck_data(response\u001b[39m.\u001b[39mjson()))\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/_http_client.py:193\u001b[0m, in \u001b[0;36mHTTPClient.call\u001b[0;34m(self, method, url, headers, params, data, json, stream, parse_response)\u001b[0m\n\u001b[1;32m    190\u001b[0m         stop_retrying()\n\u001b[1;32m    191\u001b[0m     \u001b[39mraise\u001b[39;00m ApifyApiError(response, attempt)\n\u001b[0;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m retry_with_exp_backoff(\n\u001b[1;32m    194\u001b[0m     _make_request,\n\u001b[1;32m    195\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    196\u001b[0m     backoff_base_millis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_delay_between_retries_millis,\n\u001b[1;32m    197\u001b[0m     backoff_factor\u001b[39m=\u001b[39;49mDEFAULT_BACKOFF_EXPONENTIAL_FACTOR,\n\u001b[1;32m    198\u001b[0m     random_factor\u001b[39m=\u001b[39;49mDEFAULT_BACKOFF_RANDOM_FACTOR,\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/_utils.py:64\u001b[0m, in \u001b[0;36mretry_with_exp_backoff\u001b[0;34m(func, max_retries, backoff_base_millis, backoff_factor, random_factor)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     63\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[39mreturn\u001b[39;00m func(stop_retrying, attempt)\n\u001b[1;32m     65\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m swallow:\n",
      "File \u001b[0;32m~/miniconda3/envs/datascout.venv/lib/python3.11/site-packages/apify_client/_http_client.py:191\u001b[0m, in \u001b[0;36mHTTPClient.call.<locals>._make_request\u001b[0;34m(stop_retrying, attempt)\u001b[0m\n\u001b[1;32m    189\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mStatus code is not retryable\u001b[39m\u001b[39m'\u001b[39m, extra\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mstatus_code\u001b[39m\u001b[39m'\u001b[39m: response\u001b[39m.\u001b[39mstatus_code})\n\u001b[1;32m    190\u001b[0m     stop_retrying()\n\u001b[0;32m--> 191\u001b[0m \u001b[39mraise\u001b[39;00m ApifyApiError(response, attempt)\n",
      "\u001b[0;31mApifyApiError\u001b[0m: User was not found or authentication token is not valid"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "\n",
    "# Initialize the ApifyClient with your API token\n",
    "APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "# Initialize the ApifyClient with your Apify API token\n",
    "client = ApifyClient(\"<YOUR_API_TOKEN>\")\n",
    "\n",
    "# Prepare the Actor input\n",
    "run_input = {\n",
    "    \"startUrls\": [{ \"url\": \"https://hbr.org/2012/04/the-real-leadership-lessons-of-steve-jobs\" }],\n",
    "    \"isUrlArticleDefinition\": {\n",
    "        \"minDashes\": 4,\n",
    "        \"hasDate\": True,\n",
    "        \"linkIncludes\": [\n",
    "            \"article\",\n",
    "            \"storyid\",\n",
    "            \"?p=\",\n",
    "            \"id=\",\n",
    "            \"/fpss/track\",\n",
    "            \".html\",\n",
    "            \"/content/\",\n",
    "        ],\n",
    "    },\n",
    "    \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "    \"extendOutputFunction\": \"\"\"($) => {\n",
    "    const result = {};\n",
    "    // Uncomment to add a title to the output\n",
    "    // result.pageTitle = $('title').text().trim();\n",
    "\n",
    "    return result;\n",
    "}\"\"\",\n",
    "}\n",
    "\n",
    "# Run the Actor and wait for it to finish\n",
    "run = client.actor(\"lukaskrivka/article-extractor-smart\").call(run_input=run_input)\n",
    "\n",
    "# Fetch and print Actor results from the run's dataset (if there are any)\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascout.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
