{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from apify_client import ApifyClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "from langchain_community.document_loaders.base import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "\n",
    "def google_search_actor(question):\n",
    "    # Returns a list of urls from a question on google search using apify's google search actor\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "    # prepare google search actor input\n",
    "    run_input = {\n",
    "        \"queries\": question,\n",
    "        \"maxPagesPerQuery\": 3,\n",
    "        \"resultsPerPage\": 3,\n",
    "        \"mobileResults\": False,\n",
    "        \"languageCode\": \"\",\n",
    "        \"maxConcurrency\": 10,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveHtmlToKeyValueStore\": False,\n",
    "        \"includeUnfilteredResults\": False,\n",
    "        \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
    "        return {\n",
    "        pageTitle: $('title').text(),\n",
    "        };\n",
    "    };\"\"\",\n",
    "    }\n",
    "\n",
    "    # running the actor\n",
    "    serp = client.actor(\"nFJndFXA5zjCTuudP\").call(run_input=run_input)\n",
    "\n",
    "    url_list = []\n",
    "    \n",
    "    # Creating a url list of websites for content crawling\n",
    "    for item in client.dataset(serp[\"defaultDatasetId\"]).list_items().items[0]['organicResults']:\n",
    "        if len(item) > 0:\n",
    "            url_list.append(item['url'])\n",
    "\n",
    "    print(len(url_list), \"urls\")\n",
    "\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_crawler(urls):\n",
    "    # Takes a list of urls and web scrapes using Apify's content crawler\n",
    "    # Load .env variables and initialize apifyclient with API token\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "    \n",
    "    content_crawler_input = {\n",
    "        \"startUrls\": [],\n",
    "        \"useSitemaps\": False,\n",
    "        \"crawlerType\": \"playwright:firefox\",\n",
    "        \"includeUrlGlobs\": [],\n",
    "        \"excludeUrlGlobs\": [{ \"url\": \"https://www.britannica.com\"}],\n",
    "        \"ignoreCanonicalUrl\": False,\n",
    "        \"maxCrawlDepth\": 2,\n",
    "        \"maxCrawlPages\": 4,\n",
    "        \"initialConcurrency\": 0,\n",
    "        \"maxConcurrency\": 200,\n",
    "        \"initialCookies\": [],\n",
    "        \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "        \"maxSessionRotations\": 10,\n",
    "        \"maxRequestRetries\": 3,\n",
    "        \"requestTimeoutSecs\": 60,\n",
    "        \"dynamicContentWaitSecs\": 10,\n",
    "        \"maxScrollHeightPixels\": 5000,\n",
    "        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "        [role=\\\"alert\\\"],\n",
    "        [role=\\\"banner\\\"],\n",
    "        [role=\\\"dialog\\\"],\n",
    "        [role=\\\"alertdialog\\\"],\n",
    "        [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "        [aria-modal=\\\"true\\\"]\"\"\",\n",
    "        \"removeCookieWarnings\": True,\n",
    "        \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n",
    "        \"htmlTransformer\": \"readableText\",\n",
    "        \"readableTextCharThreshold\": 100,\n",
    "        \"aggressivePrune\": False,\n",
    "        \"debugMode\": False,\n",
    "        \"debugLog\": False,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveMarkdown\": True,\n",
    "        \"saveFiles\": False,\n",
    "        \"saveScreenshots\": False,\n",
    "        \"maxResults\": 9999999,\n",
    "        \"clientSideMinChangePercentage\": 15,\n",
    "        \"renderingTypeDetectionPercentage\": 10,\n",
    "    }\n",
    "\n",
    "    for url in urls:\n",
    "        content_crawler_input[\"startUrls\"].append({\"url\": url})\n",
    "        print(\"Scraping\", url)\n",
    "\n",
    "    # Run the content crawler actor and wait for it to finish\n",
    "    website_content_crawler = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input= content_crawler_input)\n",
    "\n",
    "    # Loads the dataset into langchain document format\n",
    "    loader = ApifyDatasetLoader(\n",
    "        dataset_id= website_content_crawler[\"defaultDatasetId\"],\n",
    "        dataset_mapping_function=lambda dataset_item: Document(\n",
    "            page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_comprehension(matrix):\n",
    "    return [item for row in matrix for item in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(topic):\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    output = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content = f\"\"\"You are an AI language model assistant. Your task is to generate three \n",
    "        sets keywords of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative Google Search prompts separated by newlines. For example, if given an input of \"Steve Jobs,\" \n",
    "        you should output things like \"Steve Jobs accomplishments,\" \"Steve Jobs biography,\" and \"Steve Jobs innovations.\" Tailor\n",
    "        these prompts to he\n",
    "        lp someone who is working on an academic project, so make them intellectual. Make them broad and concise\n",
    "        enough so there is still plenty of availability for searching.\n",
    "        Query: {topic}\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    output_string = output.content\n",
    "    output_list = output_string.split('\\n')\n",
    "\n",
    "    for i in range(len(output_list)):\n",
    "        output_list[i] = output_list[i][3:]\n",
    "\n",
    "    for output in output_list:\n",
    "        print(\"Generated queries\", output)\n",
    "\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_actors(question):\n",
    "    prompts = get_prompts(question)\n",
    "\n",
    "    url_list = []\n",
    "    for prompt in prompts:\n",
    "        url_list.append(google_search_actor(prompt))\n",
    "\n",
    "    flat_url_list = flatten_comprehension(url_list)\n",
    "\n",
    "    loader = content_crawler(flat_url_list)\n",
    "\n",
    "    index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries Lebron James basketball career\n",
      "Generated queries Lebron James impact on sports\n",
      "Generated queries Lebron James philanthropy and activism\n",
      "3 urls\n",
      "2 urls\n",
      "3 urls\n",
      "Web Scraping https://en.wikipedia.org/wiki/LeBron_James\n",
      "Web Scraping https://www.britannica.com/biography/LeBron-James\n",
      "Web Scraping https://www.espn.com/nba/player/stats/_/id/1966/lebronjames\n",
      "Web Scraping https://www.espn.com/nba/story/_/id/35653907/beyond-points-winning-lebron-james-legacy-better-worse-empire\n",
      "Web Scraping https://dailyfreepress.com/2023/10/31/lebron-james-two-decades-of-dominance-on-and-off-the-court-editorial/\n",
      "Web Scraping https://www.lebronjamesfamilyfoundation.org/\n",
      "Web Scraping https://www.nba.com/news/lebron-james-off-court-legacy-complements-nba-success\n",
      "Web Scraping https://www.nytimes.com/2021/03/09/business/lebron-james-community-development.html\n",
      " LeBron James has won four championships.\n",
      "\n",
      "https://www.espn.com/nba/story/_/id/35653907/beyond-points-winning-lebron-james-legacy-better-worse-empire, https://en.wikipedia.org/wiki/LeBron_James\n"
     ]
    }
   ],
   "source": [
    "# Code for getting summaries from our dataset\n",
    "# Final function\n",
    "index = run_actors(\"Lebron James\")\n",
    "\n",
    "user_query = input(\"Enter user query: \")\n",
    "result = index.query_with_sources(user_query)\n",
    "\n",
    "print(result[\"answer\"])\n",
    "print(result[\"sources\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(x)\n",
    "\n",
    "# Stores in FAISS vector\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascout.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
