{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from apify_client import ApifyClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import ApifyDatasetLoader\n",
    "from langchain_community.document_loaders.base import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "\n",
    "def google_search_actor(question):\n",
    "    # Returns a list of urls from a question on google search using apify's google search actor\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "\n",
    "    # prepare google search actor input\n",
    "    run_input = {\n",
    "        \"queries\": question,\n",
    "        \"maxPagesPerQuery\": 3,\n",
    "        \"resultsPerPage\": 5,\n",
    "        \"mobileResults\": False,\n",
    "        \"languageCode\": \"\",\n",
    "        \"maxConcurrency\": 10,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveHtmlToKeyValueStore\": False,\n",
    "        \"includeUnfilteredResults\": False,\n",
    "        \"customDataFunction\": \"\"\"async ({ input, $, request, response, html }) => {\n",
    "        return {\n",
    "        pageTitle: $('title').text(),\n",
    "        };\n",
    "    };\"\"\",\n",
    "    }\n",
    "\n",
    "    # running the actor\n",
    "    serp = client.actor(\"nFJndFXA5zjCTuudP\").call(run_input=run_input)\n",
    "\n",
    "    url_list = []\n",
    "    \n",
    "    # Creating a url list of websites for content crawling\n",
    "    for item in client.dataset(serp[\"defaultDatasetId\"]).list_items().items[0]['organicResults']:\n",
    "        if len(item) > 0:\n",
    "            url_list.append(item['url'])\n",
    "\n",
    "    print(len(url_list), \"urls\")\n",
    "\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_crawler(urls):\n",
    "    # Takes a list of urls and web scrapes using Apify's content crawler\n",
    "    # Load .env variables and initialize apifyclient with API token\n",
    "    APIFY_API_KEY = os.getenv('APIFY_API_TOKEN')\n",
    "\n",
    "    # Initalize apify client\n",
    "    client = ApifyClient(APIFY_API_KEY)\n",
    "    \n",
    "    content_crawler_input = {\n",
    "        \"startUrls\": [],\n",
    "        \"useSitemaps\": False,\n",
    "        \"crawlerType\": \"playwright:firefox\",\n",
    "        \"includeUrlGlobs\": [],\n",
    "        \"excludeUrlGlobs\": [{ \"url\": \"https://www.britannica.com\"}],\n",
    "        \"ignoreCanonicalUrl\": False,\n",
    "        \"maxCrawlDepth\": 2,\n",
    "        \"maxCrawlPages\": 4,\n",
    "        \"initialConcurrency\": 0,\n",
    "        \"maxConcurrency\": 200,\n",
    "        \"initialCookies\": [],\n",
    "        \"proxyConfiguration\": { \"useApifyProxy\": True },\n",
    "        \"maxSessionRotations\": 10,\n",
    "        \"maxRequestRetries\": 3,\n",
    "        \"requestTimeoutSecs\": 60,\n",
    "        \"dynamicContentWaitSecs\": 10,\n",
    "        \"maxScrollHeightPixels\": 5000,\n",
    "        \"removeElementsCssSelector\": \"\"\"nav, footer, script, style, noscript, svg,\n",
    "        [role=\\\"alert\\\"],\n",
    "        [role=\\\"banner\\\"],\n",
    "        [role=\\\"dialog\\\"],\n",
    "        [role=\\\"alertdialog\\\"],\n",
    "        [role=\\\"region\\\"][aria-label*=\\\"skip\\\" i],\n",
    "        [aria-modal=\\\"true\\\"]\"\"\",\n",
    "        \"removeCookieWarnings\": True,\n",
    "        \"clickElementsCssSelector\": \"[aria-expanded=\\\"false\\\"]\",\n",
    "        \"htmlTransformer\": \"readableText\",\n",
    "        \"readableTextCharThreshold\": 100,\n",
    "        \"aggressivePrune\": False,\n",
    "        \"debugMode\": False,\n",
    "        \"debugLog\": False,\n",
    "        \"saveHtml\": False,\n",
    "        \"saveMarkdown\": True,\n",
    "        \"saveFiles\": False,\n",
    "        \"saveScreenshots\": False,\n",
    "        \"maxResults\": 9999999,\n",
    "        \"clientSideMinChangePercentage\": 15,\n",
    "        \"renderingTypeDetectionPercentage\": 10,\n",
    "    }\n",
    "\n",
    "    for url in urls:\n",
    "        content_crawler_input[\"startUrls\"].append({\"url\": url})\n",
    "        print(\"adding\", url, \"to startUrls\")\n",
    "\n",
    "    # Run the content crawler actor and wait for it to finish\n",
    "    website_content_crawler = client.actor(\"aYG0l9s7dbB7j3gbS\").call(run_input= content_crawler_input)\n",
    "\n",
    "    # Loads the dataset into langchain document format\n",
    "    loader = ApifyDatasetLoader(\n",
    "        dataset_id= website_content_crawler[\"defaultDatasetId\"],\n",
    "        dataset_mapping_function=lambda dataset_item: Document(\n",
    "            page_content=dataset_item[\"text\"], metadata={\"source\": dataset_item[\"url\"]}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(query):\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "    output = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content = f\"\"\"You are an AI language model assistant. Your task is to generate two \n",
    "        different versions of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative Google Search prompts separated by newlines.\n",
    "        Query: {query}\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output_string = output.content\n",
    "    output_list = output_string.split('\\n')\n",
    "\n",
    "    for i in range(len(output_list)):\n",
    "        output_list[i] = output_list[i][3:]\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Steve Jobs was an American businessman, inventor, and investor best known for co-founding Apple Inc. He passed away in 2011 and was posthumously awarded the Presidential Medal of Freedom in 2022. \n",
      "\n",
      "https://en.wikipedia.org/wiki/Steve_Jobs, https://www.britannica.com/biography/Steve-Jobs, https://www.apple.com/stevejobs/\n"
     ]
    }
   ],
   "source": [
    "index = content_crawler(google_search_actor(\"Steve Jobs\")[:3])\n",
    "\n",
    "\n",
    "# Code for getting summaries from our dataset\n",
    "user_query = input(\"Enter user query: \")\n",
    "result = index.query_with_sources(user_query)\n",
    "\n",
    "print(result[\"answer\"])\n",
    "print(result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_actors(query):\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "    output = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content = f\"\"\"You are an AI language model assistant. Your task is to generate two \n",
    "        different versions of the given user question to retrieve relevant documents from a vector \n",
    "        database. By generating multiple perspectives on the user question, your goal is to help\n",
    "        the user overcome some of the limitations of the distance-based similarity search. \n",
    "        Provide these alternative Google Search prompts separated by newlines.\n",
    "        Query: {query}\"\"\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    output_string = output.content\n",
    "    output_list = output_string.split('\\n')\n",
    "\n",
    "    for i in range(len(output_list)):\n",
    "        output_list[i] = output_list[i][3:]\n",
    "\n",
    "    all_url_list = []\n",
    "    \n",
    "    for question in output_list:\n",
    "        print(\"Adding query:\", question)\n",
    "        url_list.append(google_search_actor(question))\n",
    "\n",
    "    print(\"Urls:\", url_list)\n",
    "    \n",
    "    docs = content_crawler(url_list)\n",
    "    \n",
    "    return docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascout.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
